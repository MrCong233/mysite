{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "import json\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import glob\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdtree( data, divide_depth=1 ):\n",
    "    \"\"\"\n",
    "    build a kd-tree for O(n log n) nearest neighbour search\n",
    "\n",
    "    input:\n",
    "        data:       2D ndarray, shape =(ndim,ndata), preferentially C order\n",
    "        leafsize:   max. number of data points to leave in a leaf\n",
    "\n",
    "    output:\n",
    "        kd-tree:    list of tuples\n",
    "    \"\"\"\n",
    "\n",
    "    ndim = data.shape[0]\n",
    "    ndata = data.shape[1]\n",
    "\n",
    "    # find bounding hyper-rectangle\n",
    "    hrect = np.zeros((2,data.shape[0]))\n",
    "    hrect[0,:] = data.min(axis=1)\n",
    "    hrect[1,:] = data.max(axis=1)\n",
    "\n",
    "    # create root of kd-tree\n",
    "    idx = np.argsort(data[0,:], kind='mergesort')\n",
    "    data[:,:] = data[:,idx]\n",
    "    splitval = data[0,int(ndata/2)]\n",
    "\n",
    "    left_hrect = hrect.copy()\n",
    "    right_hrect = hrect.copy()\n",
    "    left_hrect[1, 0] = splitval\n",
    "    right_hrect[0, 0] = splitval\n",
    "\n",
    "    # idx, data, left_hrect, right_hrect, left_nodeptr, right_nodeptr\n",
    "    tree = [(None, None, left_hrect, right_hrect, None, None)]\n",
    "    # data, idx, depth, parent, leftbranch\n",
    "    stack = [(data[:,:int(ndata/2)], idx[:int(ndata/2)], 1, 0, True),\n",
    "             (data[:,int(ndata/2):], idx[int(ndata/2):], 1, 0, False)]\n",
    "\n",
    "    # recursively split data in halves using hyper-rectangles:\n",
    "    while stack:\n",
    "\n",
    "        # pop data off stack\n",
    "        data, didx, depth, parent, leftbranch = stack.pop()\n",
    "        ndata = data.shape[1]\n",
    "        nodeptr = len(tree)\n",
    "\n",
    "        # update parent node\n",
    "\n",
    "        _didx, _data, _left_hrect, _right_hrect, left, right = tree[parent]\n",
    "\n",
    "        tree[parent] = (_didx, _data, _left_hrect, _right_hrect, nodeptr, right) if leftbranch \\\n",
    "            else (_didx, _data, _left_hrect, _right_hrect, left, nodeptr)\n",
    "\n",
    "        # insert node in kd-tree\n",
    "\n",
    "        # leaf node?\n",
    "        if depth >= divide_depth:\n",
    "        # if ndata <= leafsize:\n",
    "            _didx = didx.copy()\n",
    "            _data = data.copy()\n",
    "            # leaf = (_didx, _data, None, None, 0, 0)\n",
    "            leaf = (_didx, _data, None, None, -1, -1)\n",
    "            tree.append(leaf)\n",
    "\n",
    "        # not a leaf, split the data in two      \n",
    "        else:\n",
    "            splitdim = depth % ndim\n",
    "            idx = np.argsort(data[splitdim,:], kind='mergesort')\n",
    "            data[:,:] = data[:,idx]\n",
    "            didx = didx[idx]\n",
    "            nodeptr = len(tree)\n",
    "            stack.append((data[:,:int(ndata/2)], didx[:int(ndata/2)], depth+1, nodeptr, True))\n",
    "            stack.append((data[:,int(ndata/2):], didx[int(ndata/2):], depth+1, nodeptr, False))\n",
    "            splitval = data[splitdim,int(ndata/2)]\n",
    "            if leftbranch:\n",
    "                left_hrect = _left_hrect.copy()\n",
    "                right_hrect = _left_hrect.copy()\n",
    "            else:\n",
    "                left_hrect = _right_hrect.copy()\n",
    "                right_hrect = _right_hrect.copy()\n",
    "            left_hrect[1, splitdim] = splitval\n",
    "            right_hrect[0, splitdim] = splitval\n",
    "            # append node to tree\n",
    "            tree.append((None, None, left_hrect, right_hrect, None, None))\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour_csv(date_, start_hour_, end_hour_):\n",
    "    # dataset_ = '20140824_train.txt'\n",
    "    dataset_ = date_ + '_train.txt'\n",
    "    dataset_hour = date_ + '_train_' + start_hour_ + '_' + end_hour_ + '.csv'\n",
    "#     dirname_ = pwd.dirname(__file__)\n",
    "#     dataset_path = dirname_ + \"\\\\dataset\\\\\" + dataset_\n",
    "    dataset_path = dataset_\n",
    "    size = 10 ** 6\n",
    "    start_time = pd.to_datetime(date_ + ' ' + start_hour_ + ':0:0')\n",
    "    end_time = pd.to_datetime(date_ + ' ' + end_hour_ + ':0:0')\n",
    "    hour_dataframe = pd.DataFrame()\n",
    "    start_chunks = timeit.default_timer()\n",
    "    for chunk in pd.read_csv(dataset_path, chunksize=size):\n",
    "        chunk.columns = ['taxi_id', 'latitude', 'longitude', 'passenger', 'time']\n",
    "        chunk.time = chunk.time.apply(pd.to_datetime)\n",
    "        mask = (chunk.time >= start_time) & (chunk.time < end_time)\n",
    "        hour_dataframe = pd.concat([hour_dataframe, chunk.loc[mask]])\n",
    "#     hour_dataframe.to_csv(dirname_ + \"\\\\dataset\\\\\" + dataset_hour, mode = 'w', index=False)\n",
    "    start_to_csv = timeit.default_timer()\n",
    "    hour_dataframe.to_csv(dataset_hour, mode = 'w', index=False)\n",
    "    end = timeit.default_timer()\n",
    "    print('chunks_time: ', start_to_csv - start_chunks)\n",
    "    print('to_csv_time: ', end - start_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks_time:  2665.8607777614693\n",
      "to_csv_time:  20.534767159665535\n"
     ]
    }
   ],
   "source": [
    "get_hour_csv('20140819', '7', '8')\n",
    "# get_hour_csv('20140819', 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour_csv_using_db(date_, start_hour_, end_hour_):\n",
    "    try:\n",
    "#         dataset_hour = date_ + '_train_' + start_hour_ + '_' + end_hour_\n",
    "        connection = psycopg2.connect(user =\"stephen_hao\",\n",
    "                                     password = \"\",\n",
    "                                     host = \"127.0.0.1\",\n",
    "                                     port = \"5432\",\n",
    "                                     database = \"urban_cross_domain_data\"\n",
    "                                     )\n",
    "        cursor = connection.cursor()\n",
    "        select_specified_hour_query = \"SELECT * FROM taxi_gps.test_train WHERE date_time >= timestamp '2014-08-03 8:00:00' and date_time < timestamp '2014-08-03 8:59:59'\"\n",
    "        start_sql = timeit.default_timer()\n",
    "        cursor.execute(select_specified_hour_query)\n",
    "        print(cursor.fetchmany(2))\n",
    "        end_sql = timeit.default_timer()\n",
    "        print('time: ', end_sql - start_sql)\n",
    "#         # Print PostgreSQL Connection properties\n",
    "#         print(connection.get_dsn_parameters(), \"\\n\")\n",
    "#         # Print PostgreSQL version\n",
    "#         cursor.execute(\"SELECT version();\")\n",
    "#         record = cursor.fetchone()\n",
    "#         print(\"You are connected to - \", record, \"\\n\")\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while connecting to PostgreSQL: \", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if (connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4445, 30.568431, 103.992946, False, datetime.datetime(2014, 8, 3, 8, 0, 32)), (4445, 30.568437, 103.992279, False, datetime.datetime(2014, 8, 3, 8, 0, 22))]\n",
      "time:  46.80571763215992\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "get_hour_csv_using_db('20140819', '7', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_od_pairs_csv(date_, start_hour_, end_hour_):\n",
    "    start = timeit.default_timer()\n",
    "    # dataset_ = '20140824_train_7_vscode.csv'\n",
    "    dataset_hour = date_ + '_train_' + start_hour_ + '_' + end_hour_ + '.csv'\n",
    "    dataset_hour_od_pairs = date_ + '_train_' + start_hour_ + '_' + end_hour_+ '_od_pairs.csv'\n",
    "#     dirname_ = pwd.dirname(__file__)\n",
    "#     dataset_path = dirname_ + \"\\\\dataset\\\\\" + dataset_hour\n",
    "    dataset_path = dataset_hour\n",
    "    hour_dataframe = pd.read_csv(dataset_path)\n",
    "    column_names = ['origin_longitude', 'origin_latitude', 'destination_longitude', 'destination_latitude']\n",
    "    up_off_pairs_df = pd.DataFrame(columns=column_names, dtype=np.float64)\n",
    "    # print(up_off_pairs_df.dtypes)\n",
    "    grouped = hour_dataframe.groupby('taxi_id', sort=False)\n",
    "    for _, group in grouped:\n",
    "        selected_group_length = len(group)\n",
    "        group = group.sort_values(by=['time'])\n",
    "        i = 0\n",
    "        while i < selected_group_length - 1:\n",
    "            if group.iloc[i]['passenger'] == np.int64(1) and group.iloc[i + 1]['passenger'] != np.int64(0):\n",
    "                up_point_row = group.iloc[i]\n",
    "                # print(up_point_row['longitude'])\n",
    "                single_pair_df = pd.DataFrame([[up_point_row['longitude'], up_point_row['latitude'], 0.0, 0.0]], columns=column_names)\n",
    "                j = i + 1\n",
    "                while j < selected_group_length:\n",
    "                    if group.iloc[j]['passenger'] == np.int64(0):\n",
    "                        off_point_row = group.iloc[j - 1]\n",
    "                        single_pair_df.iat[0, 2] = off_point_row['longitude']\n",
    "                        single_pair_df.iat[0, 3] = off_point_row['latitude']\n",
    "                        # single_pair_df.set_value(0, 'destination_longitude', off_point_row['longitude'])\n",
    "                        # single_pair_df.set_value(0, 'destination_latitude', off_point_row['latitude'])\n",
    "                        i = j + 1\n",
    "                        break\n",
    "                    else:\n",
    "                        if j == selected_group_length - 1:\n",
    "                            single_pair_df.iat[0, 2] = group.iloc[j]['longitude']\n",
    "                            single_pair_df.iat[0, 3] = group.iloc[j]['latitude']\n",
    "                            # single_pair_df.set_value(0, 'destination_longitude', group.iloc[j]['longitude'])                            \n",
    "                            # single_pair_df.set_value(0, 'destination_latitude', group.iloc[j]['latitude'])                            \n",
    "                            i = j\n",
    "                            break\n",
    "                        else:\n",
    "                            j += 1\n",
    "                if single_pair_df.iloc[0]['destination_longitude'] != np.float64(0) and single_pair_df.iloc[0]['destination_latitude'] != np.float64(0):\n",
    "                    # print(single_pair_df)\n",
    "                    # print(single_pair_df.dtypes)\n",
    "                    up_off_pairs_df = pd.concat([up_off_pairs_df, single_pair_df])\n",
    "                    # print(up_off_pairs_df)\n",
    "                    # print(up_off_pairs_df.dtypes)\n",
    "            else:\n",
    "                i += 1\n",
    "#     up_off_pairs_df.to_csv(dirname_ + \"\\\\dataset\\\\\" + dataset_hour_od_pairs, mode = 'w', index=False)\n",
    "    up_off_pairs_df.to_csv(dataset_hour_od_pairs, mode = 'w', index=False)\n",
    "    end = timeit.default_timer()\n",
    "    print('time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  477.94433512316823\n"
     ]
    }
   ],
   "source": [
    "get_od_pairs_csv('20140819', '7', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_index(leaf_tuples_list, longitude, latitude):\n",
    "    index_ = -1\n",
    "    for idx in range(len(leaf_tuples_list)):\n",
    "        nd_array = leaf_tuples_list[idx][1]\n",
    "        for i in range(nd_array.shape[1]):\n",
    "            if nd_array[0,i] == longitude and nd_array[1,i] == latitude:\n",
    "                index_ = idx\n",
    "                return index_\n",
    "    return index_\n",
    "\n",
    "def count_in_or_between_parts(date_, start_hour_, end_hour_):\n",
    "    \"\"\"\n",
    "    count in or between divided parts in specified date and hour.\n",
    "    input:\n",
    "        date_: a date format, like 20140816 and so on.\n",
    "        start_hour_: a hour format, a integer between [0, 23]\n",
    "        end_hour_: a hour format, a integer between [1, 24]\n",
    "    output:\n",
    "        part_idx_gps: [(idx, gps)], idx and gps represents the index and position of some part\n",
    "        sum_matrix: n * n matrix, sum_matrix[i, i] represents the trip times in i part,\n",
    "                    and sum_matrix[i, j] represents the trip times between i and j part.  \n",
    "    \"\"\"\n",
    "    start = timeit.default_timer()\n",
    "    dataset_hour_od_pairs = date_ + '_train_' + start_hour_ + '_' + end_hour_+ '_od_pairs.csv'\n",
    "    dataset_hour_idx_gps = date_ + '_train_' + start_hour_ + '_' + end_hour_ + '_idx_gps.json'\n",
    "    dataset_hour_sum_matrix = date_ + '_train_' + start_hour_ + '_' + end_hour_ + '_sum_matrix.json'\n",
    "#     dirname_ = pwd.dirname(__file__)\n",
    "#     dataset_path = dirname_ + \"\\\\dataset\\\\\" + dataset_hour_od_pairs\n",
    "    dataset_path = dataset_hour_od_pairs\n",
    "    up_off_pairs_df = pd.read_csv(dataset_path)\n",
    "    origin_df = up_off_pairs_df[['origin_longitude', 'origin_latitude']].copy()\n",
    "    origin_df.columns = ['longitude', 'latitude']\n",
    "    destination_df = up_off_pairs_df[['destination_longitude', 'destination_latitude']].copy()\n",
    "    destination_df.columns = ['longitude', 'latitude']\n",
    "    points_df = pd.concat([origin_df, destination_df])\n",
    "    points_array = points_df.values\n",
    "    points_T_array = points_array.transpose()\n",
    "    tree_tuples_list = kdtree(points_T_array, divide_depth=8)\n",
    "    leaf_tuples_list = [item for item in tree_tuples_list if item[4] == -1 and item[5] == -1]\n",
    "    part_idx_gps = []\n",
    "    for idx in range(len(leaf_tuples_list)):\n",
    "        single_part = (idx, (leaf_tuples_list[idx][1][0,0], leaf_tuples_list[idx][1][1,0]))\n",
    "        part_idx_gps.append(single_part)\n",
    "    # part_idx_gps = json.dumps(dict(part_idx_gps))\n",
    "    with open(dataset_hour_idx_gps, 'w') as outfile:\n",
    "        json.dump(dict(part_idx_gps), outfile)\n",
    "    # print(part_idx_gps)\n",
    "    # print(len(part_idx_gps))\n",
    "    sum_matrix =  np.zeros((len(part_idx_gps), len(part_idx_gps)), dtype=np.int)\n",
    "    start_index = timeit.default_timer()\n",
    "    for i in range(len(up_off_pairs_df)):\n",
    "        idx_up = part_index(leaf_tuples_list, up_off_pairs_df.iloc[i]['origin_longitude'], up_off_pairs_df.iloc[i]['origin_latitude'])\n",
    "        idx_off = part_index(leaf_tuples_list, up_off_pairs_df.iloc[i]['destination_longitude'], up_off_pairs_df.iloc[i]['destination_latitude'])\n",
    "        if idx_up >= 0 and idx_off >= 0:\n",
    "            sum_matrix[idx_up, idx_off] += 1\n",
    "    # print(sum_matrix)\n",
    "    end_index = timeit.default_timer()\n",
    "    print('index_time: ', end_index - start_index)\n",
    "    sum_matrix_list = sum_matrix.tolist()\n",
    "    with open(dataset_hour_sum_matrix, 'w') as output_:\n",
    "        json.dump(sum_matrix_list, output_)\n",
    "    # return part_idx_gps, sum_matrix\n",
    "    end = timeit.default_timer()\n",
    "    print('time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_time:  205.495571037648\n",
      "time:  205.69229711099433\n"
     ]
    }
   ],
   "source": [
    "count_in_or_between_parts('20140819', '7', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idx_gps_and_sum_matrix_json(date_, start_hour_, end_hour_):\n",
    "    start = timeit.default_timer()\n",
    "#     dirname_ = pwd.dirname(__file__)\n",
    "#     json_path = dirname_ + \"\\\\dataset\\\\\"\n",
    "    json_path = \"\"\n",
    "    idx_gps_json_ = json_path + date_ + '_train_' + start_hour_ + '_' + end_hour_ + '_idx_gps.json'\n",
    "    sum_matrix_json_ = json_path + date_ + '_train_' + start_hour_ + '_' + end_hour_ + '_sum_matrix.json'\n",
    "    with open(idx_gps_json_) as input_:\n",
    "        idx_gps_ = json.load(input_)\n",
    "    with open(sum_matrix_json_) as infile:\n",
    "        sum_matrix_ = json.load(infile)\n",
    "#     return idx_gps_, sum_matrix_\n",
    "    end = timeit.default_timer()\n",
    "    print('time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_idx_gps_and_sum_matrix_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-364f24db5487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_idx_gps_and_sum_matrix_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'20140819'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_idx_gps_and_sum_matrix_json' is not defined"
     ]
    }
   ],
   "source": [
    "load_idx_gps_and_sum_matrix_json('20140819', '7', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123456.jpg\n",
      "['http://www', 'baidu', 'com/python/image/123456', 'jpg']\n"
     ]
    }
   ],
   "source": [
    "#     print(glob.glob(\"E:\\\\aer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\*.txt\"))\n",
    "print(ntpath.basename('http://www.baidu.com/python/image/123456.jpg'))\n",
    "print('http://www.baidu.com/python/image/123456.jpg'.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is written in Python\n"
     ]
    }
   ],
   "source": [
    "str = \"This article is written in {}\"\n",
    "# print(str)\n",
    "print (str.format(\"Python\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n"
     ]
    }
   ],
   "source": [
    "print(\"abcd\"[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-11db195a70ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "line = str(1)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140803_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140804_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140805_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140806_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140808_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140809_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140810_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140811_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140814_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140815_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140816_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140818_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140819_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140820_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140821_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140822_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140823_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140824_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140825_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140826_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140827_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140828_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140829_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\20140830_train.txt', 'E:\\\\acer\\\\VSProjects\\\\mysite\\\\rhythm\\\\algorithm\\\\dataset\\\\predPaths_test.txt']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob(r\"E:\\acer\\VSProjects\\mysite\\rhythm\\algorithm\\dataset\\*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_to_database():\n",
    "    \"\"\"\n",
    "    input the original data which is of txt format to database.\n",
    "    \"\"\"\n",
    "    sql_of_create_tables = \"\"\n",
    "    sql_of_create_partitions = \"\"\n",
    "    sql_of_create_indices = \"\"\n",
    "    sql_of_copies = \"\"\n",
    "    sql_of_create_single_table = \"CREATE TABLE taxi_gps._{0}\\\n",
    "                                    (\\\n",
    "                                      taxi_id integer,\\\n",
    "                                      latitude double precision,\\\n",
    "                                      longitude double precision,\\\n",
    "                                      passenger boolean,\\\n",
    "                                      date_time timestamp without time zone\\\n",
    "                                    ) PARTITION BY RANGE (date_time);\"\n",
    "    sql_of_create_single_partition = \"CREATE TABLE taxi_gps._{0} PARTITION OF taxi_gps._{1}\\\n",
    "                                        FOR VALUES FROM (\\'{2}\\') TO (\\'{3}\\');\"\n",
    "    sql_of_create_single_index = \"CREATE INDEX ON taxi_gps._{0} (taxi_id);\"\n",
    "    sql_of_single_copy = \"COPY taxi_gps._{0}\\\n",
    "                                  FROM \\'{1}\\'\\\n",
    "                                  WITH DELIMITER ',';\"\n",
    "    file_paths = glob.glob(r\"E:\\acer\\VSProjects\\mysite\\rhythm\\algorithm\\dataset\\*.txt\")\n",
    "    for single_file_path in file_paths:\n",
    "        single_file_name = ntpath.basename(single_file_path)\n",
    "        file_name_list = single_file_name.split('_')\n",
    "        year = file_name_list[0][0:4]\n",
    "        month = file_name_list[0][4:6]\n",
    "        day = file_name_list[0][6:8]\n",
    "#         print(\"first: \", file_name_list)\n",
    "        if file_name_list[-1] == \"train.txt\":\n",
    "#             print(file_name_list)\n",
    "            sql_of_create_tables += sql_of_create_single_table.format(file_name_list[0])\n",
    "            hour_list = [\"00\",\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\\\n",
    "                         \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\",\\\n",
    "                         \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\"]\n",
    "            for i in range(24):\n",
    "#                 partition_name = file_name_list[0] + \"_y{}m{}d{}h{}\".format(year, month, day, str(i))\n",
    "                partition_name = file_name_list[0] + \"_h{0}\".format(hour_list[i])\n",
    "                start_time = \"{}-{}-{} {}:00:00\".format(year, month, day, i)\n",
    "                end_time = \"{}-{}-{} {}:00:00\".format(year, month, day, i + 1)\n",
    "                sql_of_create_partitions += sql_of_create_single_partition.format(partition_name, file_name_list[0], start_time, end_time)\n",
    "                sql_of_create_indices += sql_of_create_single_index.format(partition_name)\n",
    "#             print(file_name_list[0])\n",
    "            sql_of_copies += sql_of_single_copy.format(file_name_list[0], single_file_path)\n",
    "    try:\n",
    "#         dataset_hour = date_ + '_train_' + start_hour_ + '_' + end_hour_\n",
    "        connection = psycopg2.connect(user =\"postgres\",\n",
    "                                     password = \"StephenHao@3023\",\n",
    "                                     host = \"127.0.0.1\",\n",
    "                                     port = \"5432\",\n",
    "                                     database = \"urban_cross_domain_data\"\n",
    "                                     )\n",
    "        cursor = connection.cursor()\n",
    "#         select_specified_hour_query = \"SELECT * FROM taxi_gps.test_train WHERE date_time >= timestamp '2014-08-03 8:00:00' and date_time < timestamp '2014-08-03 8:59:59'\"\n",
    "        start_sql = timeit.default_timer()\n",
    "        cursor.execute(sql_of_create_tables + sql_of_create_partitions + sql_of_create_indices + sql_of_copies)\n",
    "#         print(cursor.fetchmany(2))\n",
    "        end_sql = timeit.default_timer()\n",
    "        print('time: ', end_sql - start_sql)\n",
    "#         # Print PostgreSQL Connection properties\n",
    "#         print(connection.get_dsn_parameters(), \"\\n\")\n",
    "#         # Print PostgreSQL version\n",
    "#         cursor.execute(\"SELECT version();\")\n",
    "#         record = cursor.fetchone()\n",
    "#         print(\"You are connected to - \", record, \"\\n\")\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while connecting to PostgreSQL: \", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if (connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_to_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
